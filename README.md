#Day-1
This code loads the Titanic dataset from Kaggle using pandas and begins by exploring its structure with .head(), .info(), and .describe(). A heatmap is plotted using seaborn to visualize missing data. The 'Cabin' column is dropped due to excessive missing values, while missing values in 'Age' are filled with the median and 'Embarked' with the mode. Categorical columns like 'Sex' and 'Embarked' are label-encoded or one-hot encoded appropriately. Unnecessary columns such as 'Name', 'Ticket', and 'PassengerId' are dropped to avoid irrelevant noise. Histograms and boxplots are used to visualize distributions and identify outliers, which are then removed using Z-score filtering from the scipy.stats module. Numerical features are scaled using StandardScaler, and finally, the cleaned dataset is split into training and testing sets using train_test_split, preparing it for machine learning model training with a focus on the target column 'Survived'.




#Day-2
The code begins by importing essential libraries like pandas for data manipulation, matplotlib.pyplot and seaborn for static plotting, and plotly.express for interactive visualizations. It then reads the Titanic dataset using pd.read_csv() and displays the first five rows for a quick overview. Using .info() and .describe(), it provides insights into column types, missing values, and numerical summaries. The .isnull().sum() line helps identify missing data counts per column. A heatmap from Seaborn visualizes these missing values. The code proceeds to plot a histogram of the 'Age' column using Matplotlib to view its distribution, and a Seaborn boxplot of 'Fare' grouped by 'Pclass' to observe pricing differences by class. It then uses Plotly to create an interactive pie chart of gender distribution and an interactive bar plot showing survival counts grouped by gender. Finally, it uses a Seaborn pairplot to visualize pairwise relationships between features like 'Survived', 'Pclass', 'Age', and 'Fare', helping to observe clustering and correlation patterns. These steps together allow both statistical understanding and visual exploration of the Titanic dataset.



#Day-3
This Python code uses the Kaggle House Prices dataset to demonstrate both simple and multiple linear regression. It starts by loading the train.csv file into a Pandas DataFrame and checking for missing values in the features used. For simple linear regression, only GrLivArea (above-ground living area) is used to predict SalePrice. The data is split into training and testing sets, and a linear regression model is trained and evaluated using R² score and mean squared error, with the regression line plotted for visual understanding. In the second part, multiple features—GrLivArea, TotalBsmtSF, and OverallQual—are used to predict SalePrice using multiple linear regression. The model is trained, tested, and evaluated in the same way, showing how multiple variables can enhance prediction accuracy. This code demonstrates how real-world datasets like house prices can be modeled using simple linear regression concepts and extended to multi-feature scenarios using Scikit-learn, Pandas, and Matplotlib.


#Day-4
This code builds a binary classification model using logistic regression to detect breast cancer from the Wisconsin dataset. It begins by loading the dataset and cleaning it by removing unnecessary columns and encoding the target variable (diagnosis) as 0 for benign and 1 for malignant. The features (X) and target (y) are then split into training and testing sets using an 80/20 ratio. The features are standardized using StandardScaler to improve model convergence. A logistic regression model is trained on the scaled training data, and predicted probabilities are computed on the test set. These probabilities are thresholded at 0.5 to generate class predictions, which are then evaluated using a confusion matrix, precision, recall, and ROC-AUC metrics. The ROC curve is plotted to visualize the model’s performance across thresholds. Additionally, the code allows threshold tuning (e.g., setting it to 0.3) to optimize for recall or precision based on specific needs. The sigmoid function, central to logistic regression, is used to convert the linear model’s output into probabilities between 0 and 1, enabling this threshold-based classification.

