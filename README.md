#Day-1
This code loads the Titanic dataset from Kaggle using pandas and begins by exploring its structure with .head(), .info(), and .describe(). A heatmap is plotted using seaborn to visualize missing data. The 'Cabin' column is dropped due to excessive missing values, while missing values in 'Age' are filled with the median and 'Embarked' with the mode. Categorical columns like 'Sex' and 'Embarked' are label-encoded or one-hot encoded appropriately. Unnecessary columns such as 'Name', 'Ticket', and 'PassengerId' are dropped to avoid irrelevant noise. Histograms and boxplots are used to visualize distributions and identify outliers, which are then removed using Z-score filtering from the scipy.stats module. Numerical features are scaled using StandardScaler, and finally, the cleaned dataset is split into training and testing sets using train_test_split, preparing it for machine learning model training with a focus on the target column 'Survived'.




#Day-2
The code begins by importing essential libraries like pandas for data manipulation, matplotlib.pyplot and seaborn for static plotting, and plotly.express for interactive visualizations. It then reads the Titanic dataset using pd.read_csv() and displays the first five rows for a quick overview. Using .info() and .describe(), it provides insights into column types, missing values, and numerical summaries. The .isnull().sum() line helps identify missing data counts per column. A heatmap from Seaborn visualizes these missing values. The code proceeds to plot a histogram of the 'Age' column using Matplotlib to view its distribution, and a Seaborn boxplot of 'Fare' grouped by 'Pclass' to observe pricing differences by class. It then uses Plotly to create an interactive pie chart of gender distribution and an interactive bar plot showing survival counts grouped by gender. Finally, it uses a Seaborn pairplot to visualize pairwise relationships between features like 'Survived', 'Pclass', 'Age', and 'Fare', helping to observe clustering and correlation patterns. These steps together allow both statistical understanding and visual exploration of the Titanic dataset.



#Day-3
This Python code uses the Kaggle House Prices dataset to demonstrate both simple and multiple linear regression. It starts by loading the train.csv file into a Pandas DataFrame and checking for missing values in the features used. For simple linear regression, only GrLivArea (above-ground living area) is used to predict SalePrice. The data is split into training and testing sets, and a linear regression model is trained and evaluated using R² score and mean squared error, with the regression line plotted for visual understanding. In the second part, multiple features—GrLivArea, TotalBsmtSF, and OverallQual—are used to predict SalePrice using multiple linear regression. The model is trained, tested, and evaluated in the same way, showing how multiple variables can enhance prediction accuracy. This code demonstrates how real-world datasets like house prices can be modeled using simple linear regression concepts and extended to multi-feature scenarios using Scikit-learn, Pandas, and Matplotlib.

